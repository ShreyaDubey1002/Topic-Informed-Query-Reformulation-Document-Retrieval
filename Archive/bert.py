# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cd3WksUYPoMNkBdG8_g7tXayagLUA3Yx
"""

!pip install pandas transformers

!pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git

!pip install rank_bm25

!pip install evaluate trectools

from rank_bm25 import BM25Okapi
import pandas as pd
import torch, pickle, json, csv
import warnings
import numpy as np
import evaluate

import pickle
import json
from rank_bm25 import BM25Okapi

with open("/content/document_data_5000.json") as infile:
   document_data = json.load(infile)
lst1 = [t for k, t in document_data.items()]
tokenized_corpus = [doc.split(" ") for doc in lst1]
bm25 = BM25Okapi(tokenized_corpus)
with open('/content/bm25result', 'wb') as bm25result_file:
    pickle.dump(bm25, bm25result_file)

import pandas as pd
train_topics = pd.read_csv('train_topics.csv')
train_topics['qid']=train_topics['qid'].astype(str)
mydict={}
for x in range(len(train_topics)):
  currentid = train_topics.iloc[x,0]
  t = train_topics.iloc[x,1]
  y = train_topics.iloc[x,2]
  z = train_topics.iloc[x,3]
  mydict.setdefault(currentid, [])
  mydict[currentid].append([t,y,z])

from parrot import Parrot
parrot = Parrot(model_tag="prithivida/parrot_paraphraser_on_T5")

phrase="what part of maine is portland in"
para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False, max_return_phrases = 10)
para_phrases

para_phrases

import torch, pickle, json
import warnings
from parrot import Parrot
"""
warnings.filterwarnings("ignore")
with open("/content/document_data_5000.json") as infile:
   document_data = json.load(infile)
lst1 = [t for k, t in document_data.items()]
#to read bm25 object
with open('/content/bm25result', 'rb') as bm25result_file:
    bm25 = pickle.load(bm25result_file)
"""
newqid=36
newdict = {} 
test_dict, flag = {}, 0
for qid, lis in mydict.items():
  phrase, flag = lis[0][0], 0
  doc_rel = {lis[i][1]:lis[i][2] for i in range(len(lis))}
  
  para_phrases = parrot.augment(input_phrase=phrase, use_gpu=False, max_return_phrases = 10)
  if para_phrases!=None and len(para_phrases)>2:
    flag = 1
    test_dict[phrase] = {}
  temp, c = [], 0
  for para_phrase in para_phrases:
    #print(para_phrase)
    c = 0
    #retrieve the docs for paraphrase queries
    tokenized_query = para_phrase[0].split(" ")
    docs = bm25.get_top_n(tokenized_query, lst1, n=20)

    #get the actual(original) relevance labels for the retrieved docs
    for d in docs:
      if d in doc_rel:
        temp.append([para_phrase,d,doc_rel[d]])
      else:
        c+=1
        temp.append([para_phrase,d,0])

    #add to the new_dict with a new qid
    newdict[newqid] = temp
    newqid+=1
    if flag==1:
      test_dict[phrase][para_phrase[0]] = temp
    #print("new find:",c,"total:",len(temp))

for i, k in test_dict.items():
  print(i,k)
#with open("/content/test_queries","w") as outfile:
  #json.dump(test_dict,outfile)

mydict.update(newdict)

import csv
with open('/content/train_topics_1.csv', 'w', encoding='UTF8') as f:
  writer = csv.writer(f)
  writer.writerow(['qid','query','text','label'])
  for k, v in mydict.items():
      if int(k)<36:
          for i in v:
              row = [str(k),i[0],i[1],i[2]]
              writer.writerow(row)

import pandas as pd
train_topics1 = pd.read_csv('train_topics_1.csv')
train_topics1['qid']=train_topics1['qid'].astype(str)

train_topics1.head(20)

from sklearn.model_selection import train_test_split
train_queries, val_queries, train_docs, val_docs, train_labels, val_labels = train_test_split(
    train_topics1["query"].tolist(), 
    train_topics1["text"].tolist(), 
    train_topics1["label"].tolist(), 
    test_size=.2
)

train_topics1.shape

from transformers import BertTokenizerFast
import torch

model_name = "google/bert_uncased_L-4_H-512_A-8"
tokenizer = BertTokenizerFast.from_pretrained(model_name)

train_encodings = tokenizer(train_queries, train_docs, truncation=True, padding='max_length', max_length=128)
val_encodings = tokenizer(val_queries, val_docs, truncation=True, padding='max_length', max_length=128)

class Cord19Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = Cord19Dataset(train_encodings, train_labels)
val_dataset = Cord19Dataset(val_encodings, val_labels)

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)
for param in model.base_model.parameters():
    param.requires_grad = False

import numpy as np
import evaluate

metric = evaluate.load("precision")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    print(predictions, labels)
    return metric.compute(predictions=predictions, references=labels)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy="epoch",     # Evaluation is done at the end of each epoch.
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    save_total_limit=1,              # limit the total amount of checkpoints. Deletes the older checkpoints.    
)


trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,             # evaluation dataset
    #compute_metrics=compute_metrics,
)

trainer.train()
trainer.save_model(f'out_fold{i}')

predictions = trainer.predict(val_dataset)

import pickle
# making qrel 
qrel = {}
qid, seen, i = [], {}, 0
for q in val_queries:
  if q in seen:
    qid.append(seen[q])
  else:
    seen[q] = i
    qid.append(i)
    i+=1
qrel={
    "query": qid,
    "q0": val_queries,
    "docid": val_docs,
    "rel": val_labels
}
run = {
    "query": qid,
    "q0": val_queries,
    "docid": val_docs,
    "rank": predictions[1].tolist(),
    "score": [max(l) for l in predictions[0].tolist()],
    "system": ["test" for i in qid]
}
trec_eval = evaluate.load("trec_eval")
results = trec_eval.compute(references=[qrel], predictions=[run])

with open('/content/result', 'wb') as result_file:
    pickle.dump(results,result_file)

model_name = "google/bert_uncased_L-4_H-512_A-8"
tokenizer = BertTokenizerFast.from_pretrained(model_name)
with open("/content/test_queries","r") as infile:
  test_data = json.load(infile)

for query, pp_query in test_data.items():
  for q, lst in pp_query.items:
    docs, lables = [t[1] for t in lst], [t[0] for t in lst]
    test_encodings = tokenizer(q, docs, truncation=True, padding='max_length', max_length=128)
    test_dataset = Cord19Dataset(test_encodings, labels)
    predictions = trainer.predict(test_dataset)
    print(enumerated(predictions[1].tolist()))